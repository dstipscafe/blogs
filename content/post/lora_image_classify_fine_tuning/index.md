---
title: LoRA在LLM以外的應用案例
description: 使用LoRA來微調ResNet-50好像不錯？
slug: lora_image_classify_fine_tuning
date: 2024-02-16 12:00:00+0800
image: cover_image.png
categories:
    - python
    - blogs
    - deep_learning
tags:
    - Python
    - blogs
    - LoRA
    - ResNet-50
    - Image Classification
    - Fine Tuning
    - Low-Rank Adaptation
---

## 前言

在大型語言模型如雨後春筍的推出之下，使用[LoRA](https://arxiv.org/abs/2106.09685)為大型語言模型進行微調已經是一個很普遍的技術。但LoRA是否也能為以往的一些模型微調任務提供強力的輔助，卻比較少被人所提及。我將在這篇文章中示範使用LoRA來對ResNet-50進行微調，並分析在幾種不同的組合下LoRA可以為我們帶來怎樣的便利。

{{< notice info >}}
礙於資訓練，我並不會在這篇文章中介紹LoRA的概念，如果對LoRA有興趣的話，可以參考這篇[文章](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6)。
{{< /notice >}}

## 案例探討

我將設定四種情景，來使用ResNet-50模型搭配不同的微調方式，並分析各個組合的效果。

|             項目              |             案例一            |             案例二           |            案例三          |             案例四          |
|:---------------------------: |:----------------------------:|:---------------------------:|:-------------------------:|:---------------------------:|
|  是否重新訓練ResNet-50輸出層？   |              否              |              是             |            是             |                是            |
|  是否重新修改ResNet-50輸出層？   |              否              |              否             |            否             |                是            |
|  是否有使用LoRA？               |              否             |               是            |             是             |                否            |
|  LoRA安插對象                  |              N/A            |          .*.2.conv3         |            .*.2.conv       |              N/A            |
|  是否有額外的自訂輸出層？         |              有             |              有             |              有             |              無             |
|  額外的自訂輸出層的結構           |    三層Linear+SoftMax輸出    |    三層Linear+SoftMax輸出    |    一層Linear+SoftMax輸出    |              N/A            |

### 案例設計概念

在案例一到案例三，主要探討的是在是否使用LoRA，以及在LoRA時，使用不同的輸出層的不同表現。案例四的設計目標主要為探討在不使用額外輸出層，僅重新訓練ResNet-50的輸出層時，能取得多少的準確度。

### 背景設定

* 模型： 使用Torchvision所提供的預訓練ResNet-50模型。權重為`ResNet50_Weights.IMAGENET1K_V2`。
* 資料集： 使用完整的[MNIST資料集](https://zh-yue.wikipedia.org/wiki/MNIST%E6%95%B8%E6%93%9A%E9%9B%86)進行微調。


## 成效分析

### 訓練參數量

以下表格統計了各個組合在訓練時，將會有多少參數被重新訓練。

|         項目       |             案例一            |             案例二           |            案例三          |             案例四           |
|:----------------: |:----------------------------:|:---------------------------:|:-------------------------:|:---------------------------:|
|    可訓練的參數量    |             646K            |               2.7M          |             2.1M          |               20.5K          |

### 模型準確率

首先可以藉由準確度來探討各個組合的表現。在四個組合中，案例一中表現最差，而案例二具有最好的表現。案例一的表現最差是可以理解的。在不修改ResNet-50輸出層的情況下進行微調，模型的表現不好是可預期的

值得討論的是案例三到案例四中，案例四僅重新訓練了20K左右的參數量就就可以取得0.715的準確率；而我們在使用了LoRA的前提下，對約7~8%的參數進行訓練，並取得了相當良好（0.94以上的準確率）的表現。但是從0.715到0.965之間，被重新訓練的參數量差距達到130倍以上。是否可以使用更少的參數來達到同樣的模型準確度，暫時不在這篇文章的討論範圍。

|         項目       |             案例一            |             案例二           |            案例三          |             案例四           |
|:----------------: |:----------------------------:|:---------------------------:|:-------------------------:|:---------------------------:|
|        準確率      |              0.075           |              0.965          |             0.94          |              0.715          |

## 探討

以這次的案例探討中，可以發現使用LoRA進行模型微調可以取得非常好的準確率。但除此之外，使用LoRA還有什麼好處呢？我想最重要的部分是**減少資料傳輸**。

以LoRA的微調設計而言，我們在分享模型時，不需要把整個模型都上傳到網路上，只需要將LoRA的adapter部分進行儲存，並分享即可。以本次的案例二而言，將模型整個進行儲存，檔案大小將會達到109MiB，但如果僅儲存LoRA的部分，則只需要7.9MiB的容量。相較之下，如果僅需要分享LoRA，可以減少大量的資料傳輸需求。

{{< notice note >}}
以這次的案例來說，我們有額外自訂了輸出層，所以除了LoRA的adapter部分以外，仍需要把額外的權重也分享才行。不過如果沒有額外的輸出層，就不會有這個問題存在。
{{< /notice >}}

{{< notice tip >}}
各位讀者也可以嘗試在不新增額外輸出層的前提下，僅對ResNet-50使用LoRA進行微調，重新訓練ResNet-50輸出層，藉以觀察模型的準確度變化。
{{< /notice >}}

## 小結

LoRA這個微調方法，不僅在大型語言模型的應用場景中可以帶來很大的幫助，其實用在傳統的模型微調任務中也能有強大的表現。

## 參考資料

1. [PEFT - Custom models](https://huggingface.co/docs/peft/developer_guides/custom_models)